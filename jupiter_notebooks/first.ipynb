{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 727,
   "id": "3eb65d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym_go.envs_copy import go_env\n",
    "from gym_go import gogame\n",
    "import torch \n",
    "from math import sqrt\n",
    "import time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 728,
   "id": "46b7451a",
   "metadata": {},
   "outputs": [],
   "source": [
    "board_size=7\n",
    "komi=0\n",
    "l2_loss = 1\n",
    "c_puct = 5\n",
    "N_threshold = 10\n",
    "temperature_MCTS = 2\n",
    "N_tree_search = 100\n",
    "qu_ration = 0.5\n",
    "epsilone = 0.0001\n",
    "\n",
    "# 9*9 stats\n",
    "# roll_time = 0.285\n",
    "\n",
    "# 7*7 stats\n",
    "# roll_time = 0.11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 729,
   "id": "86243ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1. 1. 1.]]\n",
      "\n",
      " [[1. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0.]]] \n",
      "----\n",
      "\n",
      "[[[1. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0.]\n",
      "  [1. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[1. 0. 0. 0. 0. 0. 0.]\n",
      "  [1. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "env = go_env.GoEnv(size=board_size, komi=komi)\n",
    "state, reward, done, info = env.step((0,0))\n",
    "print(state[:4], '\\n----\\n')\n",
    "state, reward, done, info = env.step((1,0))\n",
    "print(state[:4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 730,
   "id": "e3a0004d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutputFormating(torch.Tensor):\n",
    "    @property\n",
    "    def value(self):\n",
    "        \"\"\"Return the last element along dim=-1\"\"\"\n",
    "        if self.ndim == 1:\n",
    "            return self[-1]\n",
    "        else:\n",
    "            return self[..., -1]\n",
    "\n",
    "    @property\n",
    "    def result(self):\n",
    "        \"\"\"Return all elements except the last along dim=-1\"\"\"\n",
    "        if self.ndim == 1:\n",
    "            return self[:-1]\n",
    "        else:\n",
    "            return self[..., :-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 731,
   "id": "5eab2ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GoBot(torch.nn.Module):\n",
    "  \"\"\"\n",
    "  See only the bord as playing as the black player \n",
    "  (so when it's white we just exchange the black and white color so it can see it as black)\n",
    "  \"\"\"\n",
    "  def __init__(self):\n",
    "    super(GoBot, self).__init__()\n",
    "\n",
    "    # input channel = 1, output channels = 6, kernel size = 5\n",
    "    # input image size = (32, 32), image output size = (28, 28)\n",
    "    self.convol1 = torch.nn.Conv2d(4, 20, 7, padding=3, bias=True)\n",
    "    \n",
    "    self.convol2 = torch.nn.Conv2d(20, 40, 5, padding=2, bias=True)\n",
    "\n",
    "    self.linear1 = torch.nn.Linear(40*(board_size**2), 150, bias=True)\n",
    "\n",
    "    # input dim = 120, output dim = 84\n",
    "    self.linear2 = torch.nn.Linear(150, 110, bias=True) \n",
    "\n",
    "    self.linear3 = torch.nn.Linear(110, board_size**2 + 2, bias=True) # 83 pour les 81 case possible le suivant pass et le dernier pour la valeur\n",
    "\n",
    "\n",
    "  def forward(self, x):\n",
    "    if not isinstance(x, torch.Tensor):\n",
    "      x = torch.tensor(x, dtype=torch.float32)\n",
    "\n",
    "    if x.ndim == 3: \n",
    "        x = x.unsqueeze(0)\n",
    "\n",
    "    if x[:,2,0,0] == 1:\n",
    "      x[:, [0,1], ...] = x[:, [1,0], ...]\n",
    "    \n",
    "    x = x[:,[0,1,3,4],...]\n",
    "\n",
    "\n",
    "    x = torch.nn.ReLU() (self.convol1(x))\n",
    "\n",
    "    x = torch.nn.ReLU() (self.convol2(x))\n",
    "\n",
    "    # flatten the feature maps into a long vector\n",
    "    x = x.view(x.shape[0], -1) \n",
    "\n",
    "    x = torch.nn.ReLU() (self.linear1(x))\n",
    "    x = torch.nn.ReLU() (self.linear2(x))\n",
    "    x = self.linear3(x)\n",
    "    \n",
    "\n",
    "    return OutputFormating(x)\n",
    "  \n",
    "  @property\n",
    "  def value(self):\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb50c62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 732,
   "id": "e5765a38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OutputFormating([-0.0489,  0.0757, -0.0445,  0.0699, -0.0273,  0.0226,  0.0341,\n",
      "                  0.0289, -0.0040, -0.0984,  0.0565, -0.0251, -0.0506, -0.0877,\n",
      "                  0.0143, -0.0609, -0.0930,  0.1133,  0.0702,  0.1191,  0.0457,\n",
      "                  0.0040, -0.0841, -0.0584,  0.0612, -0.0775,  0.0550,  0.0460,\n",
      "                 -0.0260,  0.0330,  0.1022, -0.0308,  0.0442, -0.0078, -0.0046,\n",
      "                  0.0246,  0.0556, -0.0624, -0.0668,  0.0463, -0.0303,  0.0479,\n",
      "                  0.0807, -0.0888,  0.1031,  0.0872, -0.0010,  0.0763, -0.0329,\n",
      "                 -0.0259], grad_fn=<AliasBackward0>)\n",
      "OutputFormating(0.0565, grad_fn=<AliasBackward0>)\n"
     ]
    }
   ],
   "source": [
    "net = GoBot()\n",
    "out = net.forward(state)\n",
    "print(out.result[0])\n",
    "print(out.result[0,10])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 733,
   "id": "57c17516",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(output, target, reward):\n",
    "  value_loss = output.value - reward\n",
    "  policy_loss = torch.dot(target, output.result)\n",
    "  regulation = l2_loss\n",
    "  return value_loss + policy_loss + regulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 734,
   "id": "13b1cbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(net, lr, wd, momentum):\n",
    "  optimizer = torch.optim.SGD(net.parameters(), lr=lr, weight_decay=wd, momentum=momentum)\n",
    "  return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 735,
   "id": "6afcc8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(net, data_loader, cost_function, device='cuda:0'):\n",
    "  samples = 0.\n",
    "  cumulative_loss = 0.\n",
    "  cumulative_accuracy = 0.\n",
    "\n",
    "  net.eval() # Strictly needed if network contains layers which has different behaviours between train and test\n",
    "  with torch.no_grad():\n",
    "    for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
    "      # Load data into GPU\n",
    "      inputs = inputs.to(device)\n",
    "      targets = targets.to(device)\n",
    "\n",
    "      # Forward pass\n",
    "      outputs = net(inputs)\n",
    "\n",
    "      # Apply the loss\n",
    "      loss = cost_function(outputs, targets)\n",
    "\n",
    "      # Better print something\n",
    "      samples+=inputs.shape[0]\n",
    "      cumulative_loss += loss.item() # Note: the .item() is needed to extract scalars from tensors\n",
    "      _, predicted = outputs.max(1)\n",
    "      cumulative_accuracy += predicted.eq(targets).sum().item()\n",
    "\n",
    "  return cumulative_loss/samples, cumulative_accuracy/samples*100\n",
    "\n",
    "\n",
    "def train(net, data_loader, optimizer, device='cuda:0'):\n",
    "  samples = 0.\n",
    "  cumulative_loss = 0.\n",
    "\n",
    "\n",
    "  net.train() # Strictly needed if network contains layers which has different behaviours between train and test\n",
    "  for batch_idx, (state, targets, reward) in enumerate(data_loader):\n",
    "    # Load data into GPU\n",
    "    state = state.to(device)\n",
    "    targets = targets.to(device)\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = net(state)\n",
    "\n",
    "    # Apply the loss\n",
    "    loss = compute_loss(outputs, targets, reward)\n",
    "\n",
    "    # Reset the optimizer\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # Update parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Better print something, no?\n",
    "    samples+=state.shape[0]\n",
    "    cumulative_loss += loss.item()\n",
    "    # _, predicted = outputs[:82].max(1)   # for the action which has been remembered\n",
    "\n",
    "  return cumulative_loss/samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 736,
   "id": "65f6d3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def roll_policy(state, net):    \n",
    "    \"\"\"\n",
    "    return :\n",
    "        the score of the outcome of the self played game seen by the first player to play\n",
    "    \"\"\"\n",
    "    player = state[2,0,0]\n",
    "    game_ended = gogame.game_ended(state)\n",
    "\n",
    "    while not game_ended:\n",
    "        if torch.all(torch.tensor(state[3,:,:]) == 1):\n",
    "            action = board_size**2\n",
    "        else :\n",
    "            result = net([state]).result[0]\n",
    "            sorted_indices_actions = sorted(range(len(result)), key=lambda i: result[i])\n",
    "            invalid_moves = gogame.invalid_moves(state)\n",
    "            for i in range(board_size**2+1) :\n",
    "                action = sorted_indices_actions[i]\n",
    "                if not invalid_moves[action] : \n",
    "                    break\n",
    "  \n",
    "        state = gogame.next_state(state, action)\n",
    "        game_ended = gogame.game_ended(state)\n",
    "\n",
    "    score = gogame.winning(state, komi)  # from absolut black perspective\n",
    "    if player :\n",
    "        score = - score\n",
    "    return score        # from the perspective of the player who where supposed to play at the starting state of the roll_policy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 737,
   "id": "a50b73ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "state = gogame.init_state(board_size)\n",
    "print(roll_policy(state, net))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 738,
   "id": "2357b779",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = GoBot()\n",
    "\n",
    "\n",
    "class MCTS():\n",
    "    def __init__(self, state, action : int, p : torch.Tensor, depth = 0):\n",
    "        self.next_nodes = []\n",
    "        self.pass_node = None\n",
    "        self.state = state\n",
    "        self.action = action\n",
    "        self.N = 0\n",
    "        self.Wv = 0\n",
    "        self.Wr = 0\n",
    "        self.Q = 0\n",
    "        self.p = p\n",
    "        self.depth = depth\n",
    "\n",
    "    def next_state(self):\n",
    "        if self.action == None :\n",
    "            return self.state\n",
    "        else :\n",
    "            return gogame.next_state(self.state, self.action)\n",
    "    \n",
    "    def action_2d(self):\n",
    "        if self.action == None :\n",
    "            return \"None\"\n",
    "        else :\n",
    "            return self.action // board_size, self.action % board_size\n",
    "\n",
    "\n",
    "    def exctend_tree(self):\n",
    "        next_state = self.next_state()\n",
    "        result = net([next_state]).result[0]\n",
    "        for i in range(board_size):\n",
    "            for j in range(board_size):\n",
    "                if next_state[3,i,j] == 1:\n",
    "                    p = 0\n",
    "                else : \n",
    "                    p = max(epsilone,result[i*board_size + j])\n",
    "                action = i*board_size + j\n",
    "                # adding the mouve at possition (i,j)\n",
    "                self.next_nodes.append(\n",
    "                    MCTS(next_state, \n",
    "                        action, \n",
    "                        p,\n",
    "                        self.depth + 1))\n",
    "        # adding the pass mouve\n",
    "        self.next_nodes.append(\n",
    "            MCTS(next_state, \n",
    "                board_size**2, \n",
    "                epsilone,\n",
    "                self.depth + 1))\n",
    "\n",
    "\n",
    "    def push_search(self):\n",
    "        if self.N >= N_threshold :\n",
    "            if not self.next_nodes :\n",
    "                print(\"extende : \", self.action_2d(), self.depth)\n",
    "                self.exctend_tree()\n",
    "\n",
    "            max_score = 0      # all scores can be <0 so can be improved here (maybe take the score of the default pass mouve)\n",
    "            # max_score = self.next_nodes[-1].Q + c_puct * self.next_nodes[-1].p * sqrt(self.N)/(1+self.next_nodes[-1].N)\n",
    "            node_to_search = self.next_nodes[-1]    # default next mouve is the pass mouve (used if no mouves has a positive score)\n",
    "            for next_node in self.next_nodes :\n",
    "                score = next_node.Q + c_puct * next_node.p * sqrt(self.N)/(1+next_node.N)\n",
    "                # print(score, next_node)\n",
    "                if score > max_score :\n",
    "                    max_score = score\n",
    "                    node_to_search = next_node\n",
    "            value, output = node_to_search.push_search()\n",
    "            \n",
    "\n",
    "        else :\n",
    "            # print(self.action)\n",
    "            # print(self.state[3,:,:])\n",
    "            next_state = self.next_state()\n",
    "            value = net(next_state).value\n",
    "            output = roll_policy(next_state, net)\n",
    "            if self.depth % 2 == 1 :\n",
    "                value = - value\n",
    "                output = - output\n",
    "\n",
    "        self.N += 1\n",
    "        self.Wv += value\n",
    "        self.Wr += output\n",
    "        self.Q = ((1-qu_ration) * self.Wv + qu_ration * self.Wr)/self.N\n",
    "        return value, output\n",
    "    \n",
    "    def best_next_node(self):\n",
    "        max_visited_N = 0\n",
    "        i = 0\n",
    "        for next_node in self.next_nodes :\n",
    "            i += 1\n",
    "            visited_N = next_node.N\n",
    "            if visited_N > max_visited_N :\n",
    "                max_visited_N = visited_N\n",
    "                best_node = next_node\n",
    "        return best_node\n",
    "    \n",
    "    @staticmethod\n",
    "    def tree_policy(node):\n",
    "        actions = []\n",
    "        visited_N = []\n",
    "        for next_node in node.next_nodes :\n",
    "            actions.append(next_node.action)\n",
    "            visited_N.append(next_node.N)\n",
    "        diviseur = sum(N**(1/temperature_MCTS) for N in visited_N)\n",
    "        sorted_index = sorted(range(len(actions)), key=lambda i : actions[i])\n",
    "        MCTS_policy = [visited_N[i]/diviseur for i in sorted_index]\n",
    "        return MCTS_policy\n",
    "    \n",
    "    def __str__(self):\n",
    "        print(\"___ node info ___\")\n",
    "        print(\"action : {} | depth : {}\".format(self.action_2d(), self.depth))\n",
    "        print(self.N)\n",
    "        # print(self.state[3])\n",
    "        return ' '\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 739,
   "id": "bd9375e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extende :  None 0\n"
     ]
    }
   ],
   "source": [
    "state = gogame.init_state(board_size)\n",
    "root = MCTS(state, None, 1)\n",
    "for i in range(N_threshold + 3) :\n",
    "    root.push_search()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 740,
   "id": "85da99d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_one_self_play_MCTS(net):\n",
    "    data_set = []\n",
    "    state = gogame.init_state(board_size)\n",
    "    root = MCTS(state, None, 1)\n",
    "    while not gogame.game_ended(state) : \n",
    "        print(root)\n",
    "        tmp = time.time()\n",
    "        \n",
    "        for i in range(N_tree_search) : \n",
    "            root.push_search()\n",
    "        MCTS_policy = MCTS.tree_policy(root)\n",
    "        data_set.append((state,MCTS_policy))\n",
    "    \n",
    "        next_root = root.best_next_node()\n",
    "        next_state = gogame.next_state(state, next_root.action)\n",
    "        root = next_root\n",
    "        state = next_state\n",
    "        print(\"\\n\", time.time() - tmp)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97c969f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "___ node info ___\n",
      "action : None | depth : 0\n",
      "0\n",
      " \n",
      "extende :  None 0\n"
     ]
    }
   ],
   "source": [
    "root = sample_one_self_play_MCTS(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0c0073",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_self_play_MCTS_batchs(N, net, batch_size):\n",
    "  for i in range(N):\n",
    "    sample_one_self_play_MCTS(net)\n",
    "\n",
    "#   training_data = torchvision.datasets.CIFAR10('./data', train=True, transform=transform, download=True)\n",
    "  training_data = []\n",
    "\n",
    "  # Initialize dataloaders\n",
    "  train_loader = torch.utils.data.DataLoader(training_data, batch_size, shuffle=True)\n",
    "\n",
    "  return train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784a3e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Input arguments\n",
    "  batch_size: Size of a mini-batch\n",
    "  device: GPU where you want to train your network\n",
    "  weight_decay: Weight decay co-efficient for regularization of weights\n",
    "  momentum: Momentum for SGD optimizer\n",
    "  epochs: Number of epochs for training the network\n",
    "'''\n",
    "\n",
    "def main(batch_size=128,\n",
    "         device='cuda:0',\n",
    "         learning_rate=0.01,\n",
    "         weight_decay=0.000001,\n",
    "         momentum=0.9,\n",
    "         epochs=50,\n",
    "         K_early_stopping = 5):\n",
    "\n",
    "  train_loader = sample_data(batch_size, net)\n",
    "\n",
    "  net = GoBot()\n",
    "\n",
    "  optimizer = get_optimizer(net, learning_rate, weight_decay, momentum)\n",
    "\n",
    "  print('Before training:')\n",
    "  train_loss, train_accuracy = test(net, train_loader, device)\n",
    "\n",
    "\n",
    "  print('\\t Training loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
    "  print('-----------------------------------------------------')\n",
    "\n",
    "  for e in range(epochs):\n",
    "    train_loss, train_accuracy = train(net, train_loader, optimizer, device)\n",
    "    \n",
    "    print('Epoch: {:d}'.format(e+1))\n",
    "    print('\\t Training loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
    "    print('-----------------------------------------------------')\n",
    "\n",
    "  print('After training:')\n",
    "  train_loss, train_accuracy = test(net, train_loader, device)\n",
    "\n",
    "  print('\\t Training loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
    "  print('-----------------------------------------------------')\n",
    "  return net"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projet-final (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
